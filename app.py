import streamlit as st
from transformers import pipeline
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# Streamlit ì„¤ì •
st.set_page_config(page_title="ê°•ì›ë„ ê´€ê´‘ ë° ìˆ™ë°• íŠ¹í™” AI ì±—ë´‡", page_icon="ğŸŒ„")
st.title("ğŸŒ„ ê°•ì›ë„ ê´€ê´‘ ë° ìˆ™ë°• íŠ¹í™” AI ì±—ë´‡")
st.markdown("ì§ˆë¬¸ ì˜ˆì‹œ: `ì†ì´ˆ ëª…ì†Œ ì¶”ì²œí•´ì¤˜`, `ì¶˜ì²œì—ì„œ ë­˜ ë¨¹ì–´ì•¼ í•´?`, `ê°•ë¦‰ ì–´ë””ê°€ ì¢‹ì•„?`")

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
question = st.text_input("âœï¸ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

# QA íŒŒì´í”„ë¼ì¸ ìƒì„±
qa_pipeline = pipeline(
    task='question-answering',
    model='beomi/KcELECTRA-base',
    tokenizer='beomi/KcELECTRA-base',
    device=-1  # CPUì—ì„œ ê°•ì œë¡œ ì‹¤í–‰
)

# Chroma DB ë¡œë”©
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
db = Chroma(persist_directory="db", embedding_function=embedding_model)

# ì§ˆë¬¸ ì²˜ë¦¬
if question:
    with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ ì¤‘..."):
        docs = db.similarity_search(question, k=1)

    if not docs:
        st.error("ğŸ˜¥ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        context = docs[0].page_content.strip()
        if not context:
            st.warning("ë¬¸ë§¥ì´ ë¹„ì–´ ìˆì–´ ê¸°ë³¸ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            context = "ê°•ì›ë„ëŠ” ìì—°, ë°”ë‹¤, ì‚°ì´ ì–´ìš°ëŸ¬ì§„ ëŒ€í‘œ ê´€ê´‘ì§€ì…ë‹ˆë‹¤."

        with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
            result = qa_pipeline(question=question, context=context, top_k=3)
            answers = [r["answer"] for r in result]
            st.markdown("### ğŸ¤– ì±—ë´‡ì˜ ë‹µë³€")
            st.success(" Â· ".join(answers))