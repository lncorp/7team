import streamlit as st
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, pipeline

# LLM ì„¤ì •
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2")
model = GPT2LMHeadModel.from_pretrained("skt/kogpt2-base-v2")
#pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_length=100)
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=500,
    do_sample=True,
    temperature=0.7
)
llm = HuggingFacePipeline(pipeline=pipe)

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    [ì•„ë˜ëŠ” ê°•ì›ë„ ê´€ê´‘ ì •ë³´ì…ë‹ˆë‹¤. ì´ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.]
    ë¬¸ë§¥:
    {context}

    ì§ˆë¬¸:
    {question}

    ë‹µë³€:
    """
)


# ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸°
embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
db = FAISS.load_local("db", embeddings, allow_dangerous_deserialization=True)
#qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)

# Streamlit UI
st.title("ğŸ”ï¸ê°•ì›ë„ ê´€ê´‘ ë° ìˆ™ë°• íŠ¹í™” AI ì±—ë´‡")
query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")

if query:
    answer = qa.run(query)
    st.markdown(f"**ğŸ¤– ì±—ë´‡:** {answer}")
