import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ê°•ì›ë„ ê´€ê´‘ ë° ìˆ™ë°• íŠ¹í™” AI ì±—ë´‡", page_icon="ğŸŒ„")
st.title("ğŸŒ„ ê°•ì›ë„ ê´€ê´‘ ë° ìˆ™ë°• íŠ¹í™” AI ì±—ë´‡")
st.markdown("ì§ˆë¬¸ ì˜ˆì‹œ: `ì†ì´ˆ ëª…ì†Œ ì¶”ì²œí•´ì¤˜`, `ì¶˜ì²œì—ì„œ ë­˜ ë¨¹ì–´ì•¼ í•´?`, `ê°•ë¦‰ ì–´ë””ê°€ ì¢‹ì•„?`")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "qa_history" not in st.session_state:
    st.session_state.qa_history = []

# QA íŒŒì´í”„ë¼ì¸ ìƒì„±
#@st.cache_resource
#def load_qa_pipeline():
#    return pipeline(
#        task="question-answering",
#        model="beomi/KcELECTRA-base",
#        tokenizer="beomi/KcELECTRA-base",
#        device=-1  # CPU ì „ìš© ì‹¤í–‰
#    )

#qa = load_qa_pipeline()

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("beomi/KoAlpaca-Polyglot-5.8B")
    model = AutoModelForCausalLM.from_pretrained("beomi/KoAlpaca-Polyglot-5.8B")
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

qa = load_model()

# Chroma ë²¡í„° DB ë¡œë”©
@st.cache_resource
def load_chroma():
    embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
    return Chroma(persist_directory="db", embedding_function=embedding_model)

db = load_chroma()

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
#question = st.text_input("âœï¸ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")
question = st.chat_input("âœï¸ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")



# ì§ˆë¬¸ ì²˜ë¦¬
if question:
    with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ ì¤‘..."):
        docs = db.similarity_search(question, k=1)

    if not docs:
        st.error("âŒ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        context = docs[0].page_content.strip()
        with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                #result = qa(inputs=question, max_new_tokens=300)
                #output = qa(input_text, max_new_tokens=300)
                #answer = result["answer"]
                #answer = output[0]["generated_text"].strip()

                prompt = f"### ì§ˆë¬¸: {question}\n### ë¬¸ë§¥: {context}\n### ë‹µë³€:"

                output = qa(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)
                answer = output[0]["generated_text"].split("### ë‹µë³€:")[-1].strip()

                messages = st.container(height=100)
                messages2 = st.container(height=400)

                # ì¶œë ¥
                #st.markdown("### ğŸ¤– ì±—ë´‡ì˜ ë‹µë³€")
                #st.success(answer)
                #st.markdown("#### ğŸ” ì°¸ê³  ë¬¸ë§¥")
                #st.info(context)

                messages.chat_message("user").write(question)
                messages2.chat_message("assistant").write(f"ì±—ë´‡ì˜ ë‹µë³€: {answer}")

                # íˆìŠ¤í† ë¦¬ ì €ì¥
                st.session_state.qa_history.append((question, answer))

            except Exception as e:
                st.error(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì´ì „ ëŒ€í™” ê¸°ë¡ ë³´ê¸° / ë‹¤ìš´ë¡œë“œ / ì´ˆê¸°í™”
if st.session_state.qa_history:
    with st.expander("ğŸ—ƒï¸ ì´ì „ ì§ˆë¬¸ê³¼ ë‹µë³€ ë³´ê¸° / ì €ì¥ / ì´ˆê¸°í™”"):
        for i, (q, a) in enumerate(reversed(st.session_state.qa_history), 1):
            st.markdown(f"**Q{i}:** {q}")
            st.markdown(f"**A{i}:** {a}")
            st.markdown("---")

        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.download_button(
            label="ğŸ“¥ ì „ì²´ ëŒ€í™” ì €ì¥ (txt)",
            data="\n\n".join([f"Q: {q}\nA: {a}" for q, a in st.session_state.qa_history]),
            file_name="qa_history.txt",
            mime="text/plain"
        )

        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ§¹ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.qa_history.clear()
            st.success("âœ… ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")