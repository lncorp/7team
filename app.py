import streamlit as st
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, pipeline

# LLM ì„¤ì •
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2")
model = GPT2LMHeadModel.from_pretrained("skt/kogpt2-base-v2")
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_length=100)
llm = HuggingFacePipeline(pipeline=pipe)

# ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸°
embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
db = FAISS.load_local("faiss_gangwon_db", embeddings)
qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

# Streamlit UI
st.title("ğŸ”ï¸ ê°•ì›ë„ ê´€ê´‘ ì•ˆë‚´ ì±—ë´‡ (LangChain + RAG)")
query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")

if query:
    answer = qa.run(query)
    st.markdown(f"**ğŸ¤– ì±—ë´‡:** {answer}")
