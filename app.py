import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ê°•ì›ë„ ê´€ê´‘ ë° ìˆ™ë°• íŠ¹í™” AI ì±—ë´‡", page_icon="ğŸŒ„")
st.title("ğŸŒ„ ê°•ì›ë„ ê´€ê´‘ ë° ìˆ™ë°• íŠ¹í™” AI ì±—ë´‡")
st.markdown("ì§ˆë¬¸ ì˜ˆì‹œ: `ì†ì´ˆ ëª…ì†Œ ì¶”ì²œí•´ì¤˜`, `ì¶˜ì²œì—ì„œ ë­˜ ë¨¹ì–´ì•¼ í•´?`, `ê°•ë¦‰ ì–´ë””ê°€ ì¢‹ì•„?`")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# KcELECTRA(ì¼€ì´ì”¨ ì¼ë ‰íŠ¸ë¼) ëª¨ë¸ ë¡œë”©
#@st.cache_resource
#def load_qa_pipeline():
#    return pipeline(
#        task="question-answering",
#        model="beomi/KcELECTRA-base",
#        tokenizer="beomi/KcELECTRA-base",
#        device=-1  # CPU ì „ìš© ì‹¤í–‰
#    )

#qa = load_qa_pipeline()

# KoAlpaca(ì½”ì•ŒíŒŒì¹´) ëª¨ë¸ ë¡œë”©
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("beomi/KoAlpaca-Polyglot-5.8B")
    model = AutoModelForCausalLM.from_pretrained("beomi/KoAlpaca-Polyglot-5.8B")
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

qa = load_model()

# Chroma ë²¡í„° DB ë¡œë”©
@st.cache_resource
def load_chroma():
    embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-sts")
    return Chroma(persist_directory="db", embedding_function=embedding_model)

db = load_chroma()

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
question = st.chat_input("âœï¸ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")


# ì§ˆë¬¸ ì²˜ë¦¬
if question:
    with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ ì¤‘..."):
        docs = db.similarity_search(question, k=1)

    if not docs:
        st.chat_message("assistant").write("âŒ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        context = docs[0].page_content.strip()
        with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                prompt = f"### ì§ˆë¬¸: {question}\n### ë¬¸ë§¥: {context}\n### ë‹µë³€:"
                output = qa(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)
                answer = output[0]["generated_text"].split("### ë‹µë³€:")[-1].strip()

                st.chat_message("user").write(question)
                st.chat_message("assistant").write(answer)

                # íˆìŠ¤í† ë¦¬ ì €ì¥
                st.session_state.chat_history.append(("ì§ˆë¬¸", question))
                st.session_state.chat_history.append(("ë‹µë³€", answer))

            except Exception as e:
                st.chat_message("assistant").write(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì´ì „ ëŒ€í™” ê¸°ë¡ ë³´ê¸° / ë‹¤ìš´ë¡œë“œ / ì´ˆê¸°í™”
if st.session_state.chat_history:
    with st.expander("ğŸ—ƒï¸ ì´ì „ ì§ˆë¬¸ê³¼ ë‹µë³€ ë³´ê¸° / ì €ì¥ / ì´ˆê¸°í™”"):
        for role, msg in st.session_state.chat_history:
            st.markdown(f"**{role.upper()}**: {msg}")
        st.download_button(
            label="ğŸ“¥ ì „ì²´ ëŒ€í™” ì €ì¥ (txt)",
            data="\n\n".join([f"{role.upper()}: {msg}" for role, msg in st.session_state.chat_history]),
            file_name="chat_history.txt",
            mime="text/plain"
        )
        if st.button("ğŸ§¹ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.chat_history.clear()
            st.success("âœ… ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")